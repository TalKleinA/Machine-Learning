{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sure Tomorrow insurance company wants to solve several tasks with the help of Machine Learning and you are asked to evaluate that possibility.\n",
    "\n",
    "- Task 1: Find customers who are similar to a given customer. This will help the company's agents with marketing.\n",
    "- Task 2: Predict whether a new customer is likely to receive an insurance benefit. Can a prediction model do better than a dummy model?\n",
    "- Task 3: Predict the number of insurance benefits a new customer is likely to receive using a linear regression model.\n",
    "- Task 4: Protect clients' personal data without breaking the model from the previous task. It's necessary to develop a data transformation algorithm that would make it hard to recover personal information if the data fell into the wrong hands. This is called data masking, or data obfuscation. But the data should be protected in such a way that the quality of machine learning models doesn't suffer. You don't need to pick the best model, just prove that the algorithm works correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing & Exploration\n",
    "\n",
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.2 MB 142 kB/s eta 0:00:01     |██████████████████▋             | 13.5 MB 2.1 MB/s eta 0:00:05\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.14.6 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.19.5)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "Successfully installed scikit-learn-1.0.1 threadpoolctl-3.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as math\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import seaborn as sns\n",
    "try:\n",
    "    import sidetable\n",
    "except:\n",
    "    ! pip install sidetable -Uq\n",
    "    import sidetable\n",
    "    \n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import sklearn.neighbors\n",
    "import sklearn.preprocessing\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "       warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and conduct a basic check that it's free from obvious issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv('/datasets/insurance_us.csv')\n",
    "except:\n",
    "    df = pd.read_csv('insurance_us.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rename the colums to make the code look more consistent with its style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'Gender': 'gender', 'Age': 'age', 'Salary': 'income', 'Family members': 'family_members', 'Insurance benefits': 'insurance_benefits'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>family_members</th>\n",
       "      <th>insurance_benefits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2852</td>\n",
       "      <td>0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>35100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1551</td>\n",
       "      <td>1</td>\n",
       "      <td>42.0</td>\n",
       "      <td>40500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3472</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>16700.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2864</td>\n",
       "      <td>1</td>\n",
       "      <td>37.0</td>\n",
       "      <td>45100.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2096</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>40500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>722</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>40200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1759</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35100.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1686</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>31600.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2249</td>\n",
       "      <td>1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>548</td>\n",
       "      <td>1</td>\n",
       "      <td>33.0</td>\n",
       "      <td>41500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      gender   age   income  family_members  insurance_benefits\n",
       "2852       0  34.0  35100.0               0                   0\n",
       "1551       1  42.0  40500.0               0                   1\n",
       "3472       0  22.0  16700.0               0                   0\n",
       "2864       1  37.0  45100.0               2                   0\n",
       "2096       0  23.0  40500.0               3                   0\n",
       "722        0  25.0  40200.0               1                   0\n",
       "1759       0  35.0  35100.0               2                   0\n",
       "1686       1  20.0  31600.0               2                   0\n",
       "2249       1  19.0  30000.0               1                   0\n",
       "548        1  33.0  41500.0               0                   0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 5 columns):\n",
      "gender                5000 non-null int64\n",
      "age                   5000 non-null float64\n",
      "income                5000 non-null float64\n",
      "family_members        5000 non-null int64\n",
      "insurance_benefits    5000 non-null int64\n",
      "dtypes: float64(2), int64(3)\n",
      "memory usage: 195.4 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we may want to fix the age type (from float to int) though this is not critical\n",
    "df['age'] = df.age.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 5 columns):\n",
      "gender                5000 non-null int64\n",
      "age                   5000 non-null int64\n",
      "income                5000 non-null float64\n",
      "family_members        5000 non-null int64\n",
      "insurance_benefits    5000 non-null int64\n",
      "dtypes: float64(1), int64(4)\n",
      "memory usage: 195.4 KB\n"
     ]
    }
   ],
   "source": [
    "# check to see that the conversion was successful\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>family_members</th>\n",
       "      <th>insurance_benefits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.499000</td>\n",
       "      <td>30.952800</td>\n",
       "      <td>39916.360000</td>\n",
       "      <td>1.194200</td>\n",
       "      <td>0.148000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.500049</td>\n",
       "      <td>8.440807</td>\n",
       "      <td>9900.083569</td>\n",
       "      <td>1.091387</td>\n",
       "      <td>0.463183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>5300.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>33300.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>40200.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>46600.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>79000.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            gender          age        income  family_members  \\\n",
       "count  5000.000000  5000.000000   5000.000000     5000.000000   \n",
       "mean      0.499000    30.952800  39916.360000        1.194200   \n",
       "std       0.500049     8.440807   9900.083569        1.091387   \n",
       "min       0.000000    18.000000   5300.000000        0.000000   \n",
       "25%       0.000000    24.000000  33300.000000        0.000000   \n",
       "50%       0.000000    30.000000  40200.000000        1.000000   \n",
       "75%       1.000000    37.000000  46600.000000        2.000000   \n",
       "max       1.000000    65.000000  79000.000000        6.000000   \n",
       "\n",
       "       insurance_benefits  \n",
       "count         5000.000000  \n",
       "mean             0.148000  \n",
       "std              0.463183  \n",
       "min              0.000000  \n",
       "25%              0.000000  \n",
       "50%              0.000000  \n",
       "75%              0.000000  \n",
       "max              5.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now have a look at the data's descriptive statistics. \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gender splits almost evenly between the values 1 and 0. Age is ranging from 18 YO to 65 with mean and median equal both to ~30. Income has the biggest scale with range from 5,300 to 79,000 with STD of 9900!. family members range from 1 to 6 with half of the population have 1 or less. The target insurance_benefits distributes from 0 to 5 where the majority have no benefits at all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n"
     ]
    }
   ],
   "source": [
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>family_members</th>\n",
       "      <th>insurance_benefits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>48100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>488</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>32900.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>513</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>37400.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>718</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>32600.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>785</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>35800.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4793</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>37800.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4902</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>38700.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4935</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>32700.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4945</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>45800.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4965</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>40100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      gender  age   income  family_members  insurance_benefits\n",
       "281        1   39  48100.0               1                   0\n",
       "488        1   24  32900.0               1                   0\n",
       "513        0   31  37400.0               2                   0\n",
       "718        1   22  32600.0               1                   0\n",
       "785        0   20  35800.0               0                   0\n",
       "...      ...  ...      ...             ...                 ...\n",
       "4793       1   24  37800.0               0                   0\n",
       "4902       1   35  38700.0               1                   0\n",
       "4935       1   19  32700.0               0                   0\n",
       "4945       1   21  45800.0               0                   0\n",
       "4965       0   22  40100.0               1                   0\n",
       "\n",
       "[153 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 152 duplicated rows which are not neccesserily duplicate since there is no unique identifoer to the row or customer. In addition this amount isn't big so it's better to leave them as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_1c870fc8_3db3_11ec_be02_02420a394b41\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >missing</th>        <th class=\"col_heading level0 col1\" >total</th>        <th class=\"col_heading level0 col2\" >percent</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_1c870fc8_3db3_11ec_be02_02420a394b41level0_row0\" class=\"row_heading level0 row0\" >gender</th>\n",
       "                        <td id=\"T_1c870fc8_3db3_11ec_be02_02420a394b41row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "                        <td id=\"T_1c870fc8_3db3_11ec_be02_02420a394b41row0_col1\" class=\"data row0 col1\" >5,000</td>\n",
       "                        <td id=\"T_1c870fc8_3db3_11ec_be02_02420a394b41row0_col2\" class=\"data row0 col2\" >0.00%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c870fc8_3db3_11ec_be02_02420a394b41level0_row1\" class=\"row_heading level0 row1\" >age</th>\n",
       "                        <td id=\"T_1c870fc8_3db3_11ec_be02_02420a394b41row1_col0\" class=\"data row1 col0\" >0</td>\n",
       "                        <td id=\"T_1c870fc8_3db3_11ec_be02_02420a394b41row1_col1\" class=\"data row1 col1\" >5,000</td>\n",
       "                        <td id=\"T_1c870fc8_3db3_11ec_be02_02420a394b41row1_col2\" class=\"data row1 col2\" >0.00%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c870fc8_3db3_11ec_be02_02420a394b41level0_row2\" class=\"row_heading level0 row2\" >income</th>\n",
       "                        <td id=\"T_1c870fc8_3db3_11ec_be02_02420a394b41row2_col0\" class=\"data row2 col0\" >0</td>\n",
       "                        <td id=\"T_1c870fc8_3db3_11ec_be02_02420a394b41row2_col1\" class=\"data row2 col1\" >5,000</td>\n",
       "                        <td id=\"T_1c870fc8_3db3_11ec_be02_02420a394b41row2_col2\" class=\"data row2 col2\" >0.00%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c870fc8_3db3_11ec_be02_02420a394b41level0_row3\" class=\"row_heading level0 row3\" >family_members</th>\n",
       "                        <td id=\"T_1c870fc8_3db3_11ec_be02_02420a394b41row3_col0\" class=\"data row3 col0\" >0</td>\n",
       "                        <td id=\"T_1c870fc8_3db3_11ec_be02_02420a394b41row3_col1\" class=\"data row3 col1\" >5,000</td>\n",
       "                        <td id=\"T_1c870fc8_3db3_11ec_be02_02420a394b41row3_col2\" class=\"data row3 col2\" >0.00%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c870fc8_3db3_11ec_be02_02420a394b41level0_row4\" class=\"row_heading level0 row4\" >insurance_benefits</th>\n",
       "                        <td id=\"T_1c870fc8_3db3_11ec_be02_02420a394b41row4_col0\" class=\"data row4 col0\" >0</td>\n",
       "                        <td id=\"T_1c870fc8_3db3_11ec_be02_02420a394b41row4_col1\" class=\"data row4 col1\" >5,000</td>\n",
       "                        <td id=\"T_1c870fc8_3db3_11ec_be02_02420a394b41row4_col2\" class=\"data row4 col2\" >0.00%</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f1cd7062fd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.stb.missing(style=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " No missing values. yay :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly check whether there are certain groups of customers by looking at the pair plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = sns.pairplot(df, kind='hist')\n",
    "g.fig.set_size_inches(12, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, it is a bit difficult to spot obvious groups (clusters) as it is difficult to combine several variables simultaneously (to analyze multivariate distributions). That's where LA and ML can be quite handy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1. Similar Customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the language of ML, it is necessary to develop a procedure that returns k nearest neighbors (objects) for a given object based on the distance between the objects.\n",
    "\n",
    "You may want to review the following lessons (chapter -> lesson)\n",
    "- Distance Between Vectors -> Euclidean Distance\n",
    "- Distance Between Vectors -> Manhattan Distance\n",
    "\n",
    "To solve the task, we can try different distance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that returns k nearest neighbors for an $n^{th}$ object based on a specified distance metric. The number of received insurance benefits should not be taken into account for this task. \n",
    "\n",
    "You can use a ready implementation of the kNN algorithm from scikit-learn (check [the link](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors)) or use your own.\n",
    "\n",
    "Test it for four combination of two cases\n",
    "- Scaling\n",
    "  - the data is not scaled\n",
    "  - the data is scaled with the [MaxAbsScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html) scaler\n",
    "- Distance Metrics\n",
    "  - Euclidean\n",
    "  - Manhattan\n",
    "\n",
    "Answer these questions:\n",
    "- Does the data being not scaled affect the kNN algorithm? If so, how does that appear?\n",
    "- How similar are the results using the Manhattan distance metric (regardless of the scaling)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['gender', 'age', 'income', 'family_members']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knn(df, n, k, metric):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns k nearest neighbors\n",
    "\n",
    "    :param df: pandas DataFrame used to find similar objects within\n",
    "    :param n: object no for which the nearest neighbours are looked for\n",
    "    :param k: the number of the nearest neighbours to return\n",
    "    :param metric: name of distance metric\n",
    "    \"\"\"\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, metric=metric)\n",
    "    nbrs.fit(df[feature_names])\n",
    "    nbrs_distances, nbrs_indices = nbrs.kneighbors([df.iloc[n][feature_names]], k, return_distance=True)\n",
    "    \n",
    "    df_res = pd.concat([\n",
    "        df.iloc[nbrs_indices[0]], \n",
    "        pd.DataFrame(nbrs_distances.T, index=nbrs_indices[0], columns=['distance'])\n",
    "        ], axis=1)\n",
    "    \n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = get_knn(df,6,5,'euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      gender  age   income  family_members  insurance_benefits  distance\n",
      "6          1   39  39700.0               2                   0  0.000000\n",
      "4248       1   38  39700.0               1                   0  1.414214\n",
      "3695       1   40  39700.0               3                   0  1.414214\n",
      "1704       0   39  39700.0               0                   0  2.236068\n",
      "4977       1   41  39700.0               0                   0  2.828427\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['gender', 'age', 'income', 'family_members']\n",
    "\n",
    "transformer_mas = sklearn.preprocessing.MaxAbsScaler().fit(df[feature_names].to_numpy())\n",
    "\n",
    "df_scaled = df.copy()\n",
    "df_scaled.loc[:, feature_names] = transformer_mas.transform(df[feature_names].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>family_members</th>\n",
       "      <th>insurance_benefits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1335</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.353846</td>\n",
       "      <td>0.597468</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.426582</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.415385</td>\n",
       "      <td>0.465823</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.646154</td>\n",
       "      <td>0.426582</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.676923</td>\n",
       "      <td>0.436709</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      gender       age    income  family_members  insurance_benefits\n",
       "1335     1.0  0.353846  0.597468        0.000000                   0\n",
       "3138     0.0  0.538462  0.426582        0.333333                   0\n",
       "242      0.0  0.415385  0.465823        0.333333                   0\n",
       "70       1.0  0.646154  0.426582        0.166667                   1\n",
       "4704     0.0  0.676923  0.436709        0.166667                   1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get similar records for a given one for every combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      gender  age   income  family_members  insurance_benefits  distance\n",
      "6          1   39  39700.0               2                   0  0.000000\n",
      "3695       1   40  39700.0               3                   0  1.414214\n",
      "4248       1   38  39700.0               1                   0  1.414214\n",
      "1704       0   39  39700.0               0                   0  2.236068\n",
      "4977       1   41  39700.0               0                   0  2.828427\n",
      "1253       1   34  39700.0               2                   0  5.000000\n",
      "4362       1   45  39700.0               2                   1  6.000000\n",
      "413        1   45  39700.0               0                   1  6.324555\n",
      "350        0   33  39700.0               0                   0  6.403124\n",
      "410        0   32  39700.0               1                   0  7.141428\n"
     ]
    }
   ],
   "source": [
    "#combination: euclidean distance no scaling\n",
    "print(get_knn(df,6,10,'euclidean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      gender       age    income  family_members  insurance_benefits  distance\n",
      "6        1.0  0.600000  0.502532        0.333333                   0  0.000000\n",
      "1415     1.0  0.600000  0.505063        0.333333                   0  0.002532\n",
      "108      1.0  0.600000  0.507595        0.333333                   0  0.005063\n",
      "3713     1.0  0.600000  0.517722        0.333333                   0  0.015190\n",
      "2997     1.0  0.584615  0.503797        0.333333                   0  0.015437\n",
      "4434     1.0  0.600000  0.478481        0.333333                   0  0.024051\n",
      "951      1.0  0.600000  0.474684        0.333333                   0  0.027848\n",
      "4680     1.0  0.615385  0.478481        0.333333                   0  0.028550\n",
      "3706     1.0  0.584615  0.477215        0.333333                   0  0.029624\n",
      "77       1.0  0.615385  0.475949        0.333333                   0  0.030713\n"
     ]
    }
   ],
   "source": [
    "#combination: euclidean distance with scaling\n",
    "print(get_knn(df_scaled,6,10,'euclidean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      gender  age   income  family_members  insurance_benefits  distance\n",
      "6          1   39  39700.0               2                   0       0.0\n",
      "3695       1   40  39700.0               3                   0       2.0\n",
      "4248       1   38  39700.0               1                   0       2.0\n",
      "1704       0   39  39700.0               0                   0       3.0\n",
      "4977       1   41  39700.0               0                   0       4.0\n",
      "1253       1   34  39700.0               2                   0       5.0\n",
      "4362       1   45  39700.0               2                   1       6.0\n",
      "413        1   45  39700.0               0                   1       8.0\n",
      "410        0   32  39700.0               1                   0       9.0\n",
      "350        0   33  39700.0               0                   0       9.0\n"
     ]
    }
   ],
   "source": [
    "#combination: manhattan distance no scaling\n",
    "print(get_knn(df,6,10,'manhattan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      gender       age    income  family_members  insurance_benefits  distance\n",
      "6        1.0  0.600000  0.502532        0.333333                   0  0.000000\n",
      "1415     1.0  0.600000  0.505063        0.333333                   0  0.002532\n",
      "108      1.0  0.600000  0.507595        0.333333                   0  0.005063\n",
      "3713     1.0  0.600000  0.517722        0.333333                   0  0.015190\n",
      "2997     1.0  0.584615  0.503797        0.333333                   0  0.016650\n",
      "4434     1.0  0.600000  0.478481        0.333333                   0  0.024051\n",
      "951      1.0  0.600000  0.474684        0.333333                   0  0.027848\n",
      "4680     1.0  0.615385  0.478481        0.333333                   0  0.039435\n",
      "3706     1.0  0.584615  0.477215        0.333333                   0  0.040701\n",
      "77       1.0  0.615385  0.475949        0.333333                   0  0.041967\n"
     ]
    }
   ],
   "source": [
    "#combination: manhattan distance with scaling\n",
    "print(get_knn(df_scaled,6,10,'manhattan'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with a different record:  117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      gender  age   income  family_members  insurance_benefits   distance\n",
      "117        1   52  32000.0               0                   2   0.000000\n",
      "1315       0   48  32000.0               1                   1   4.242641\n",
      "4925       1   44  32000.0               1                   1   8.062258\n",
      "3914       1   39  32000.0               1                   0  13.038405\n",
      "3813       1   36  32000.0               2                   0  16.124515\n",
      "3828       0   32  32000.0               0                   0  20.024984\n",
      "1687       0   32  32000.0               2                   0  20.124612\n",
      "3910       0   30  32000.0               0                   0  22.022716\n",
      "2173       1   29  32000.0               4                   0  23.345235\n",
      "1434       0   26  32000.0               2                   0  26.095977\n"
     ]
    }
   ],
   "source": [
    "#combination: euclidean distance no scaling\n",
    "print(get_knn(df,117,10,'euclidean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      gender       age    income  family_members  insurance_benefits  distance\n",
      "117      1.0  0.800000  0.405063             0.0                   2  0.000000\n",
      "95       1.0  0.800000  0.393671             0.0                   2  0.011392\n",
      "4838     1.0  0.784615  0.375949             0.0                   2  0.032929\n",
      "1692     1.0  0.784615  0.443038             0.0                   2  0.040973\n",
      "339      1.0  0.830769  0.448101             0.0                   2  0.052906\n",
      "859      1.0  0.753846  0.474684             0.0                   2  0.083529\n",
      "4970     1.0  0.769231  0.484810             0.0                   2  0.085477\n",
      "4632     1.0  0.830769  0.489873             0.0                   2  0.090219\n",
      "1222     1.0  0.707692  0.408861             0.0                   1  0.092386\n",
      "2052     1.0  0.753846  0.500000             0.0                   2  0.105561\n"
     ]
    }
   ],
   "source": [
    "#combination: euclidean distance with scaling\n",
    "print(get_knn(df_scaled,117,10,'euclidean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      gender  age   income  family_members  insurance_benefits  distance\n",
      "117        1   52  32000.0               0                   2       0.0\n",
      "1315       0   48  32000.0               1                   1       6.0\n",
      "4925       1   44  32000.0               1                   1       9.0\n",
      "3914       1   39  32000.0               1                   0      14.0\n",
      "3813       1   36  32000.0               2                   0      18.0\n",
      "3828       0   32  32000.0               0                   0      21.0\n",
      "1687       0   32  32000.0               2                   0      23.0\n",
      "3910       0   30  32000.0               0                   0      23.0\n",
      "2173       1   29  32000.0               4                   0      27.0\n",
      "113        0   25  32000.0               0                   0      28.0\n"
     ]
    }
   ],
   "source": [
    "#combination: manhattan distance no scaling\n",
    "print(get_knn(df,117,10,'manhattan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      gender       age    income  family_members  insurance_benefits  distance\n",
      "117      1.0  0.800000  0.405063             0.0                   2  0.000000\n",
      "95       1.0  0.800000  0.393671             0.0                   2  0.011392\n",
      "4838     1.0  0.784615  0.375949             0.0                   2  0.044499\n",
      "1692     1.0  0.784615  0.443038             0.0                   2  0.053359\n",
      "339      1.0  0.830769  0.448101             0.0                   2  0.073807\n",
      "1222     1.0  0.707692  0.408861             0.0                   1  0.096105\n",
      "1568     1.0  0.907692  0.406329             0.0                   4  0.108958\n",
      "4970     1.0  0.769231  0.484810             0.0                   2  0.110516\n",
      "4632     1.0  0.830769  0.489873             0.0                   2  0.115579\n",
      "859      1.0  0.753846  0.474684             0.0                   2  0.115774\n"
     ]
    }
   ],
   "source": [
    "#combination: manhattan distance with scaling\n",
    "print(get_knn(df_scaled,117,10,'manhattan'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers to the questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Does the data being not scaled affect the kNN algorithm? If so, how does that appear?** \n",
    "\n",
    "The scaling improves the model a great deal. No matter the distance matric, the scaling returns a different set of neighbors. Without acaling the income feature is a great influencer on the neighbors picking. After the scaling other features are more similiar to each other like family status, age and gender. It happens because the scale of the sincome is very big compared to other features and hence it affects badly in the model cuase flatuations in income create a big distance between two observations. After scaling other features can have greater weights, more balanced between all features, in choosing neighbors in a way that describes the reality better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How similar are the results using the Manhattan distance metric (regardless of the scaling)?** \n",
    "\n",
    "The results of different distance metrics are not that different. Only when choosing a big amount of k neighbors, like 10, we can see a liitle different results berween the 2 metrics. Nevertheless after scaling those differences are not seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2. Is Customer Likely to Receive Insurance Benefit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of machine learning we can look at this like a binary classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `insurance_benefits` being more than zero as the target, evaluate whether the kNN classification approach can do better than a dummy model.\n",
    "\n",
    "Instructions:\n",
    "- Build a KNN-based classifier and measure its quality with the F1 metric for k=1..10 for both the original data and the scaled one. That'd be interesting to see how k may influece the evaluation metric, and whether scaling the data makes any difference. You can use a ready implemention of the kNN classification algorithm from scikit-learn (check [the link](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)) or use your own.\n",
    "- Build the dummy model which is just random for this case. It should return \"1\" with some probability. Let's test the model with four probability values: 0, the probability of paying any insurance benefit, 0.5, 1.\n",
    "\n",
    "The probability of paying any insurance benefit can be defined as\n",
    "\n",
    "$$\n",
    "P\\{\\text{insurance benefit received}\\}=\\frac{\\text{number of clients received any insurance benefit}}{\\text{total number of clients}}.\n",
    "$$\n",
    "\n",
    "Split the whole data in the 70:30 proportion for the training/testing parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the target\n",
    "\n",
    "df['insurance_benefits_received'] = np.where(df['insurance_benefits']>0, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.8872\n",
       "1    0.1128\n",
       "Name: insurance_benefits_received, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for the class imbalance with value_counts()\n",
    "\n",
    "df['insurance_benefits_received'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "ratio = round(0.8872 / 0.1128)\n",
    "print(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target classes are very imbalanced. 88.7% of the customers don't have any insurance benefits. We'de better use some balancing tools to help our model become more accurtae and not neglecting the minority class - 1. Let's apply upsampling on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(features, target, repeat):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n",
    "    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n",
    "\n",
    "    features_upsampled, target_upsampled = shuffle(\n",
    "        features_upsampled, target_upsampled, random_state=12345\n",
    "    )\n",
    "\n",
    "    return features_upsampled, target_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_classifier(y_true, y_pred):\n",
    "    \n",
    "    f1_score = sklearn.metrics.f1_score(y_true, y_pred)\n",
    "    print(f'F1: {f1_score:.2f}')\n",
    "    \n",
    "# if you have an issue with the following line, restart the kernel and run the notebook again\n",
    "    cm = sklearn.metrics.confusion_matrix(y_true, y_pred, normalize='all')\n",
    "    print('Confusion Matrix')\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating output of a random model\n",
    "\n",
    "def rnd_model_predict(P, size, seed=42):\n",
    "\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    return rng.binomial(n=1, p=P, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability: 0.00\n",
      "F1: 0.00\n",
      "Confusion Matrix\n",
      "[[0.8872 0.    ]\n",
      " [0.1128 0.    ]]\n",
      "\n",
      "The probability: 0.11\n",
      "F1: 0.12\n",
      "Confusion Matrix\n",
      "[[0.7914 0.0958]\n",
      " [0.0994 0.0134]]\n",
      "\n",
      "The probability: 0.50\n",
      "F1: 0.20\n",
      "Confusion Matrix\n",
      "[[0.456  0.4312]\n",
      " [0.053  0.0598]]\n",
      "\n",
      "The probability: 1.00\n",
      "F1: 0.20\n",
      "Confusion Matrix\n",
      "[[0.     0.8872]\n",
      " [0.     0.1128]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for P in [0, df['insurance_benefits_received'].sum() / len(df), 0.5, 1]:\n",
    "\n",
    "    print(f'The probability: {P:.2f}')\n",
    "    y_pred_rnd = rnd_model_predict(P, len(df))\n",
    "        \n",
    "    eval_classifier(df['insurance_benefits_received'], y_pred_rnd)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random model generated a very low F1 score of 0.0 with probability 0 since in this case the recall and percision are equal to 0 since there are no True Positives at all. The higher the P the more 1s are predicted and hence the F1 score is bigger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data to train:test 70:30\n",
    "train, test = train_test_split(df, test_size=0.3, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train.drop(['insurance_benefits_received','insurance_benefits'], axis=1)\n",
    "train_target = train['insurance_benefits_received']\n",
    "test_features = test.drop(['insurance_benefits_received','insurance_benefits'], axis=1)\n",
    "test_target = test['insurance_benefits_received']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upsampling to balance the data\n",
    "train_features, train_target = upsample(train_features, train_target, ratio)\n",
    "test_features, test_target = upsample(test_features, test_target, ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " k: 1, distance: manhattan \n",
      "\n",
      "K Neighbors F1 Score on Test Set:\n",
      "F1: 0.70\n",
      "Confusion Matrix\n",
      "[[0.49450966 0.01173798]\n",
      " [0.22415752 0.26959485]]\n",
      "\n",
      " k: 1, distance: euclidean \n",
      "\n",
      "K Neighbors F1 Score on Test Set:\n",
      "F1: 0.68\n",
      "Confusion Matrix\n",
      "[[0.49450966 0.01173798]\n",
      " [0.23324498 0.26050738]]\n",
      "\n",
      " k: 2, distance: manhattan \n",
      "\n",
      "K Neighbors F1 Score on Test Set:\n",
      "F1: 0.69\n",
      "Confusion Matrix\n",
      "[[0.4948883  0.01135933]\n",
      " [0.22718667 0.26656569]]\n",
      "\n",
      " k: 2, distance: euclidean \n",
      "\n",
      "K Neighbors F1 Score on Test Set:\n",
      "F1: 0.67\n",
      "Confusion Matrix\n",
      "[[0.49450966 0.01173798]\n",
      " [0.23627414 0.25747823]]\n",
      "\n",
      " k: 3, distance: manhattan \n",
      "\n",
      "K Neighbors F1 Score on Test Set:\n",
      "F1: 0.72\n",
      "Confusion Matrix\n",
      "[[0.47860659 0.02764105]\n",
      " [0.19992427 0.2938281 ]]\n",
      "\n",
      " k: 3, distance: euclidean \n",
      "\n",
      "K Neighbors F1 Score on Test Set:\n",
      "F1: 0.72\n",
      "Confusion Matrix\n",
      "[[0.48049981 0.02574782]\n",
      " [0.20295343 0.29079894]]\n",
      "\n",
      " k: 4, distance: manhattan \n",
      "\n",
      "K Neighbors F1 Score on Test Set:\n",
      "F1: 0.72\n",
      "Confusion Matrix\n",
      "[[0.47898523 0.0272624 ]\n",
      " [0.19992427 0.2938281 ]]\n",
      "\n",
      " k: 4, distance: euclidean \n",
      "\n",
      "K Neighbors F1 Score on Test Set:\n",
      "F1: 0.72\n",
      "Confusion Matrix\n",
      "[[0.48087846 0.02536918]\n",
      " [0.20295343 0.29079894]]\n",
      "\n",
      " k: 5, distance: manhattan \n",
      "\n",
      "K Neighbors F1 Score on Test Set:\n",
      "F1: 0.73\n",
      "Confusion Matrix\n",
      "[[0.45815979 0.04808785]\n",
      " [0.18174934 0.31200303]]\n",
      "\n",
      " k: 5, distance: euclidean \n",
      "\n",
      "K Neighbors F1 Score on Test Set:\n",
      "F1: 0.74\n",
      "Confusion Matrix\n",
      "[[0.45815979 0.04808785]\n",
      " [0.17872018 0.31503218]]\n",
      "\n",
      " k: 6, distance: manhattan \n",
      "\n",
      "K Neighbors F1 Score on Test Set:\n",
      "F1: 0.73\n",
      "Confusion Matrix\n",
      "[[0.45815979 0.04808785]\n",
      " [0.18174934 0.31200303]]\n",
      "\n",
      " k: 6, distance: euclidean \n",
      "\n",
      "K Neighbors F1 Score on Test Set:\n",
      "F1: 0.74\n",
      "Confusion Matrix\n",
      "[[0.45815979 0.04808785]\n",
      " [0.17872018 0.31503218]]\n",
      "\n",
      " k: 7, distance: manhattan \n",
      "\n",
      "K Neighbors F1 Score on Test Set:\n",
      "F1: 0.73\n",
      "Confusion Matrix\n",
      "[[0.43468383 0.0715638 ]\n",
      " [0.16660356 0.32714881]]\n",
      "\n",
      " k: 7, distance: euclidean \n",
      "\n",
      "K Neighbors F1 Score on Test Set:\n",
      "F1: 0.74\n",
      "Confusion Matrix\n",
      "[[0.43506248 0.07118516]\n",
      " [0.1635744  0.33017796]]\n",
      "\n",
      " k: 8, distance: manhattan \n",
      "\n",
      "K Neighbors F1 Score on Test Set:\n",
      "F1: 0.73\n",
      "Confusion Matrix\n",
      "[[0.43468383 0.0715638 ]\n",
      " [0.16660356 0.32714881]]\n",
      "\n",
      " k: 8, distance: euclidean \n",
      "\n",
      "K Neighbors F1 Score on Test Set:\n",
      "F1: 0.74\n",
      "Confusion Matrix\n",
      "[[0.43506248 0.07118516]\n",
      " [0.1635744  0.33017796]]\n",
      "\n",
      " k: 9, distance: manhattan \n",
      "\n",
      "K Neighbors F1 Score on Test Set:\n",
      "F1: 0.73\n",
      "Confusion Matrix\n",
      "[[0.40742143 0.0988262 ]\n",
      " [0.15448694 0.33926543]]\n",
      "\n",
      " k: 9, distance: euclidean \n",
      "\n",
      "K Neighbors F1 Score on Test Set:\n",
      "F1: 0.73\n",
      "Confusion Matrix\n",
      "[[0.40893601 0.09731162]\n",
      " [0.15448694 0.33926543]]\n",
      "\n",
      " k: 10, distance: manhattan \n",
      "\n",
      "K Neighbors F1 Score on Test Set:\n",
      "F1: 0.73\n",
      "Confusion Matrix\n",
      "[[0.40817872 0.09806891]\n",
      " [0.15448694 0.33926543]]\n",
      "\n",
      " k: 10, distance: euclidean \n",
      "\n",
      "K Neighbors F1 Score on Test Set:\n",
      "F1: 0.73\n",
      "Confusion Matrix\n",
      "[[0.40893601 0.09731162]\n",
      " [0.15448694 0.33926543]]\n"
     ]
    }
   ],
   "source": [
    "for k in range(1,11):\n",
    "    for metric in ['manhattan','euclidean']:\n",
    "        print('\\n k: {}, distance: {} \\n'.format(k,metric))\n",
    "        model = KNeighborsClassifier(n_neighbors=k, metric=metric)\n",
    "        model.fit(train_features, train_target)\n",
    "        train_predict = model.predict(train_features)\n",
    "        \n",
    "        test_predict = model.predict(test_features)\n",
    "        print(\"K Neighbors F1 Score on Test Set:\")\n",
    "        eval_classifier(test_target, test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best F1 score is 0.74 for the following combinations:\n",
    "k: 5, distance: euclidean \n",
    "k: 6, distance: euclidean \n",
    "k: 7, distance: euclidean \n",
    "k: 8, distance: euclidean \n",
    "\n",
    "It means that euclidean distance measure performs better in KNN model than manhattan and the optimal number of neighbors are 5-8. I guess that less neighbors are prowned to anomalies and hence less precise and big amount of neighbors  have some extra instances thatare not similiar enough to the predicted instance and affect badly on the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's scale the data and run the KNN model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['gender', 'age', 'income', 'family_members']\n",
    "\n",
    "transformer_mas = sklearn.preprocessing.MaxAbsScaler().fit(train_features.to_numpy())\n",
    "\n",
    "train_features_scaled = train_features.copy()\n",
    "train_features_scaled = transformer_mas.transform(train_features.to_numpy())\n",
    "\n",
    "test_features_scaled = test_features.copy()\n",
    "test_features_scaled = transformer_mas.transform(test_features.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " k: 1, distance: manhattan \n",
      "\n",
      "F1: 0.98\n",
      "Confusion Matrix\n",
      "[[0.50473306 0.00151458]\n",
      " [0.02120409 0.47254828]]\n",
      "\n",
      " k: 1, distance: euclidean \n",
      "\n",
      "F1: 0.98\n",
      "Confusion Matrix\n",
      "[[0.50473306 0.00151458]\n",
      " [0.02120409 0.47254828]]\n",
      "\n",
      " k: 2, distance: manhattan \n",
      "\n",
      "F1: 0.98\n",
      "Confusion Matrix\n",
      "[[0.50473306 0.00151458]\n",
      " [0.02120409 0.47254828]]\n",
      "\n",
      " k: 2, distance: euclidean \n",
      "\n",
      "F1: 0.98\n",
      "Confusion Matrix\n",
      "[[0.50473306 0.00151458]\n",
      " [0.02120409 0.47254828]]\n",
      "\n",
      " k: 3, distance: manhattan \n",
      "\n",
      "F1: 0.99\n",
      "Confusion Matrix\n",
      "[[0.50056797 0.00567967]\n",
      " [0.00605831 0.48769406]]\n",
      "\n",
      " k: 3, distance: euclidean \n",
      "\n",
      "F1: 0.98\n",
      "Confusion Matrix\n",
      "[[0.49981068 0.00643696]\n",
      " [0.01514578 0.47860659]]\n",
      "\n",
      " k: 4, distance: manhattan \n",
      "\n",
      "F1: 0.99\n",
      "Confusion Matrix\n",
      "[[0.50056797 0.00567967]\n",
      " [0.00605831 0.48769406]]\n",
      "\n",
      " k: 4, distance: euclidean \n",
      "\n",
      "F1: 0.98\n",
      "Confusion Matrix\n",
      "[[0.49981068 0.00643696]\n",
      " [0.01514578 0.47860659]]\n",
      "\n",
      " k: 5, distance: manhattan \n",
      "\n",
      "F1: 0.99\n",
      "Confusion Matrix\n",
      "[[0.49526694 0.01098069]\n",
      " [0.00302916 0.49072321]]\n",
      "\n",
      " k: 5, distance: euclidean \n",
      "\n",
      "F1: 0.98\n",
      "Confusion Matrix\n",
      "[[0.49337372 0.01287391]\n",
      " [0.00605831 0.48769406]]\n",
      "\n",
      " k: 6, distance: manhattan \n",
      "\n",
      "F1: 0.99\n",
      "Confusion Matrix\n",
      "[[0.49526694 0.01098069]\n",
      " [0.00302916 0.49072321]]\n",
      "\n",
      " k: 6, distance: euclidean \n",
      "\n",
      "F1: 0.98\n",
      "Confusion Matrix\n",
      "[[0.49337372 0.01287391]\n",
      " [0.00605831 0.48769406]]\n",
      "\n",
      " k: 7, distance: manhattan \n",
      "\n",
      "F1: 0.99\n",
      "Confusion Matrix\n",
      "[[0.4914805  0.01476713]\n",
      " [0.         0.49375237]]\n",
      "\n",
      " k: 7, distance: euclidean \n",
      "\n",
      "F1: 0.98\n",
      "Confusion Matrix\n",
      "[[0.48996592 0.01628171]\n",
      " [0.00302916 0.49072321]]\n",
      "\n",
      " k: 8, distance: manhattan \n",
      "\n",
      "F1: 0.99\n",
      "Confusion Matrix\n",
      "[[0.4914805  0.01476713]\n",
      " [0.         0.49375237]]\n",
      "\n",
      " k: 8, distance: euclidean \n",
      "\n",
      "F1: 0.98\n",
      "Confusion Matrix\n",
      "[[0.48996592 0.01628171]\n",
      " [0.00302916 0.49072321]]\n",
      "\n",
      " k: 9, distance: manhattan \n",
      "\n",
      "F1: 0.98\n",
      "Confusion Matrix\n",
      "[[0.48920863 0.017039  ]\n",
      " [0.         0.49375237]]\n",
      "\n",
      " k: 9, distance: euclidean \n",
      "\n",
      "F1: 0.98\n",
      "Confusion Matrix\n",
      "[[0.48731541 0.01893222]\n",
      " [0.         0.49375237]]\n",
      "\n",
      " k: 10, distance: manhattan \n",
      "\n",
      "F1: 0.98\n",
      "Confusion Matrix\n",
      "[[0.48920863 0.017039  ]\n",
      " [0.         0.49375237]]\n",
      "\n",
      " k: 10, distance: euclidean \n",
      "\n",
      "F1: 0.98\n",
      "Confusion Matrix\n",
      "[[0.48731541 0.01893222]\n",
      " [0.         0.49375237]]\n"
     ]
    }
   ],
   "source": [
    "for k in range(1,11):\n",
    "    for metric in ['manhattan','euclidean']:\n",
    "        print('\\n k: {}, distance: {} \\n'.format(k,metric))\n",
    "        model = KNeighborsClassifier(n_neighbors=k, metric=metric)\n",
    "        model.fit(train_features_scaled, train_target)\n",
    "        \n",
    "        test_predict = model.predict(test_features_scaled)\n",
    "        test_score = eval_classifier(test_target, test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 score is much higher after the scailing. It went up to 0.98 and 0.99. It's very high!\n",
    "The best combinations here are:\n",
    "k: 8, distance: manhattan \n",
    "k: 7, distance: manhattan \n",
    "k: 6, distance: manhattan \n",
    "k: 5, distance: manhattan \n",
    "k: 4, distance: manhattan \n",
    "k: 3, distance: manhattan \n",
    "\n",
    "It seems that after the scaling the better distance matric is manhattan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3. Regression (with Linear Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `insurance_benefits` as the target, evaluate what RMSE would be for a Linear Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build your own implementation of LR. For that, recall how the linear regression task's solution is formulated in terms of LA. Check RMSE for both the original data and the scaled one. Can you see any difference in RMSE between these two cases?\n",
    "\n",
    "Let's denote\n",
    "- $X$ — feature matrix, each row is a case, each column is a feature, the first column consists of unities\n",
    "- $y$ — target (a vector)\n",
    "- $\\hat{y}$ — estimated tagret (a vector)\n",
    "- $w$ — weight vector\n",
    "\n",
    "The task of linear regression in the language of matrices can be formulated as\n",
    "\n",
    "$$\n",
    "y = Xw\n",
    "$$\n",
    "\n",
    "The training objective then is to find such $w$ that it would minimize the L2-distance (MSE) between $Xw$ and $y$:\n",
    "\n",
    "$$\n",
    "\\min_w d_2(Xw, y) \\quad \\text{or} \\quad \\min_w \\text{MSE}(Xw, y)\n",
    "$$\n",
    "\n",
    "It appears that there is analytical solution for the above:\n",
    "\n",
    "$$\n",
    "w = (X^T X)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "The formula above can be used to find the weights $w$ and the latter can be used to calculate predicted values\n",
    "\n",
    "$$\n",
    "\\hat{y} = X_{val}w\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the whole data in the 70:30 proportion for the training/validation parts. Use the RMSE metric for the model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearRegression:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.weights = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # adding the unities\n",
    "        X2 = np.append(np.ones([len(X), 1]), X, axis=1)\n",
    "        self.weights = np.linalg.inv(X2.T @ X2) @ X2.T @ y\n",
    "#         self.weights = weights[1:]\n",
    "#         self.w0 = weights[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        # adding the unities\n",
    "        X2 = np.append(np.ones([len(X), 1]), X, axis=1)\n",
    "        y_pred = X2.dot(self.weights)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_regressor(y_true, y_pred):\n",
    "    \n",
    "    rmse = math.sqrt(sklearn.metrics.mean_squared_error(y_true, y_pred))\n",
    "    print(f'RMSE: {rmse:.2f}')\n",
    "    \n",
    "    r2_score = math.sqrt(sklearn.metrics.r2_score(y_true, y_pred))\n",
    "    print(f'R2: {r2_score:.2f}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-9.43539012e-01  3.57495491e-02  1.64272726e-02 -2.60743659e-07\n",
      " -1.16902127e-02]\n",
      "RMSE: 0.34\n",
      "R2: 0.66\n"
     ]
    }
   ],
   "source": [
    "X = df[['age', 'gender', 'income', 'family_members']].to_numpy()\n",
    "y = df['insurance_benefits'].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345)\n",
    "\n",
    "lr = MyLinearRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "print(lr.weights)\n",
    "\n",
    "y_test_pred = lr.predict(X_test)\n",
    "eval_regressor(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_mas = sklearn.preprocessing.MaxAbsScaler().fit(X_train)\n",
    "\n",
    "# x_train_scaled = X_train.copy()\n",
    "x_train_scaled = transformer_mas.transform(X_train)\n",
    "\n",
    "# x_test_scaled = x_test.copy()\n",
    "x_test_scaled = transformer_mas.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.94353901  2.32372069  0.01642727 -0.02059875 -0.07014128]\n",
      "RMSE: 0.34\n",
      "R2: 0.66\n"
     ]
    }
   ],
   "source": [
    "lr.fit(x_train_scaled, y_train)\n",
    "print(lr.weights)\n",
    "\n",
    "y_test_pred = lr.predict(x_test_scaled)\n",
    "eval_regressor(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE and F1 evaluations metrics haven't changed at all after scaling using the MaxAbsScaler.\n",
    "It seems that linear regression modle isn't sensitive to scaling like KNN for example. The reason is that unlike distance-based models, in linear regression there are weights, that can counteract the difference of scales. Scaling can still be useful if we want to interpret the weights, or if we use L1 or L2 regularization, which use distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4. Obfuscating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It best to obfuscate data by multiplying the numerical features (remember, they can be seen as the matrix $X$) by an invertible matrix $P$. \n",
    "\n",
    "$$\n",
    "X' = X \\times P\n",
    "$$\n",
    "\n",
    "Try to do that and check how the features' values will look like after the transformation. By the way, the intertible property is important here so make sure that $P$ is indeed invertible.\n",
    "\n",
    "You may want to review the 'Matrices and Matrix Operations -> Matrix Mupliplication' lesson to recall the rule of matrix multiplication and its implementation with NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_info_column_list = ['gender', 'age', 'income', 'family_members']\n",
    "df_pn = df[personal_info_column_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_pn.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating a random matrix $P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=42)\n",
    "P = rng.random(size=(X.shape[1], X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the matrix $P$ is invertible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.41467992 -1.43783972  0.62798546  1.14001268]\n",
      " [-1.06101789  0.44219337  0.1329549   1.18425933]\n",
      " [ 1.42362442  1.60461607 -2.0553823  -1.53699695]\n",
      " [-0.11128575 -0.65813802  1.74995517 -0.11816316]]\n"
     ]
    }
   ],
   "source": [
    "p_invers = np.linalg.inv(P)\n",
    "print(p_invers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you guess the customers' ages or income after the transformation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6359.71527314 22380.40467609 18424.09074184 46000.69669016]\n",
      " [ 4873.29406479 17160.36702982 14125.78076133 35253.45577301]\n",
      " [ 2693.11742928  9486.397744    7808.83156024 19484.86063067]\n",
      " ...\n",
      " [ 4346.2234249  15289.24126492 12586.16264392 31433.50888552]\n",
      " [ 4194.09324155 14751.9910242  12144.02930637 30323.88763426]\n",
      " [ 5205.46827354 18314.24814446 15077.01370762 37649.59295455]]\n"
     ]
    }
   ],
   "source": [
    "x_obfus = X.dot(P)\n",
    "print(x_obfus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obfuscation changes the matrix a lot. The scale of the values is much higher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you recover the original data from $X'$ if you know $P$? Try to check that with calculations by moving $P$ from the right side of the formula above to the left one. The rules of matrix multiplcation are really helpful here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yes we can multiply by the invers of P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+00  4.10000000e+01  4.96000000e+04  1.00000000e+00]\n",
      " [ 1.67952800e-12  4.60000000e+01  3.80000000e+04  1.00000000e+00]\n",
      " [-6.23021448e-13  2.90000000e+01  2.10000000e+04 -2.03032656e-13]\n",
      " ...\n",
      " [ 1.57996161e-12  2.00000000e+01  3.39000000e+04  2.00000000e+00]\n",
      " [ 1.00000000e+00  2.20000000e+01  3.27000000e+04  3.00000000e+00]\n",
      " [ 1.00000000e+00  2.80000000e+01  4.06000000e+04  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "original_x = x_obfus.dot(p_invers)\n",
    "print(original_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks similiar but not exactly the same. Let's compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print all three cases for a few customers\n",
    "- The original data\n",
    "- The transformed one\n",
    "- The reversed (recovered) one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original data:\n",
      "[[1.00e+00 4.20e+01 4.02e+04 0.00e+00]\n",
      " [1.00e+00 3.90e+01 3.17e+04 1.00e+00]\n",
      " [1.00e+00 4.20e+01 6.92e+04 0.00e+00]\n",
      " [0.00e+00 2.00e+01 3.31e+04 1.00e+00]\n",
      " [0.00e+00 3.00e+01 3.16e+04 2.00e+00]\n",
      " [0.00e+00 2.40e+01 3.04e+04 1.00e+00]\n",
      " [0.00e+00 2.60e+01 3.45e+04 1.00e+00]\n",
      " [0.00e+00 4.20e+01 3.87e+04 1.00e+00]\n",
      " [1.00e+00 2.50e+01 3.31e+04 1.00e+00]\n",
      " [0.00e+00 2.80e+01 3.66e+04 1.00e+00]]\n",
      "The transformed one:\n",
      "[[ 5154.89743822 18146.92972061 14938.90703955 37289.66462056]\n",
      " [ 4066.29289355 14316.54514306 11785.28382867 29410.03126116]\n",
      " [ 8870.19278581 31208.12191958 25692.0497423  64165.84929717]\n",
      " [ 4243.08865364 14928.10975299 12289.08081034 30691.86965572]\n",
      " [ 4052.50384322 14263.10983128 11740.93858521 29309.81005422]\n",
      " [ 3897.55855481 13715.97021008 11290.97070372 28192.74844305]\n",
      " [ 4423.01280347 15564.50380015 12812.76488247 31994.05702594]\n",
      " [ 4962.59689827 17471.73469694 14382.29481948 35899.04700798]\n",
      " [ 4244.33349643 14933.42674319 12293.74510677 30696.49734527]\n",
      " [ 4692.23978679 16512.26551444 13592.96301277 33941.83563113]]\n",
      "The reversed (recovered) one:\n",
      "[[ 1.00000000e+00  4.20000000e+01  4.02000000e+04 -3.93160481e-12]\n",
      " [ 1.00000000e+00  3.90000000e+01  3.17000000e+04  1.00000000e+00]\n",
      " [ 1.00000000e+00  4.20000000e+01  6.92000000e+04  1.26325763e-12]\n",
      " [-1.03823251e-12  2.00000000e+01  3.31000000e+04  1.00000000e+00]\n",
      " [ 1.63016903e-12  3.00000000e+01  3.16000000e+04  2.00000000e+00]\n",
      " [ 1.53876314e-12  2.40000000e+01  3.04000000e+04  1.00000000e+00]\n",
      " [ 1.23456191e-12  2.60000000e+01  3.45000000e+04  1.00000000e+00]\n",
      " [ 1.83143765e-12  4.20000000e+01  3.87000000e+04  1.00000000e+00]\n",
      " [ 1.00000000e+00  2.50000000e+01  3.31000000e+04  1.00000000e+00]\n",
      " [ 2.19523964e-12  2.80000000e+01  3.66000000e+04  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print('The original data:')\n",
    "print(X[40:50])\n",
    "print('The transformed one:')\n",
    "print(x_obfus[40:50])\n",
    "print('The reversed (recovered) one:')\n",
    "print(original_x[40:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert to int to see how really differnret the valsues are from each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original data:\n",
      "[[    1    42 40200     0]\n",
      " [    1    39 31700     1]\n",
      " [    1    42 69200     0]\n",
      " [    0    20 33100     1]\n",
      " [    0    30 31600     2]\n",
      " [    0    24 30400     1]\n",
      " [    0    26 34500     1]\n",
      " [    0    42 38700     1]\n",
      " [    1    25 33100     1]\n",
      " [    0    28 36600     1]]\n",
      "The reversed (recovered) one:\n",
      "[[    1    41 40200     0]\n",
      " [    0    38 31700     1]\n",
      " [    1    41 69200     0]\n",
      " [    0    20 33100     1]\n",
      " [    0    30 31600     1]\n",
      " [    0    24 30399     0]\n",
      " [    0    26 34500     0]\n",
      " [    0    42 38700     1]\n",
      " [    1    25 33099     0]\n",
      " [    0    28 36599     1]]\n"
     ]
    }
   ],
   "source": [
    "print('The original data:')\n",
    "print(X[40:50].astype(int))\n",
    "print('The reversed (recovered) one:')\n",
    "print(original_x[40:50].astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can probably see that some values are not exactly the same as they are in the original data. What might be the reason for that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: The memory of the computer is limited and hence the computer rounds the outcome of each procedure and chnage it a little bit. limitations of floating point arithmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof That Data Obfuscation Can Work with LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression task has been solved with linear regression in this project. Your next task is to prove _analytically_ that the given obfuscation method won't affect linear regression in terms of predicted values i.e. their values will remain the same. Can you believe that? Well, you don't have to, you should prove it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the data is obfuscated and there is $X \\times P$ instead of just $X$ now. Consequently, there are other weights $w_P$ as\n",
    "$$\n",
    "w = (X^T X)^{-1} X^T y \\quad \\Rightarrow \\quad w_P = [(XP)^T XP]^{-1} (XP)^T y\n",
    "$$\n",
    "\n",
    "How would $w$ and $w_P$ be linked if you simplify the formula for $w_P$ above? \n",
    "\n",
    "What would be predicted values with $w_P$? \n",
    "\n",
    "What does that mean for the quality of linear regression if you measure it with RMSE?\n",
    "\n",
    "Check Appendix B Properties of Matrices in the end of the notebook. There are useful formulas in there!\n",
    "\n",
    "No code is necessary in this section, only analytical explanation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How would $w$ and $w_P$ be linked if you simplify the formula for $w_P$ above? \n",
    "\n",
    "$Pw_P = w$\n",
    "\n",
    "or\n",
    "\n",
    "$w_P = [P]^{-1} w$\n",
    "\n",
    "when applying P transformation on $w_P$ we get w.\n",
    "\n",
    "Alternatively when applying $[P]^{-1}$ transformation on w we get $w_P$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analytical proof**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ w = (X^T X)^{-1} X^T y $  \n",
    "$ y = X w $\n",
    "\n",
    "$$ w_P = [(XP)^T XP]^{-1} (XP)^T y $$\n",
    "apply $(AB)^{-1} = B^{-1}A^{-1}$\n",
    "$$ \\Rightarrow w_P = [P^T X^T XP]^{-1} P^T X^T y $$\n",
    "apply Associative property of multiplication\n",
    "$$ \\Rightarrow w_P = P^{-1} (X^T X)^{-1} (P^T)^{-1} P^T X^T y $$\n",
    "apply $A^{-1}A = AA^{-1} = I$\n",
    "$$ \\Rightarrow w_P = P^{-1} (X^T X)^{-1} I X^T y $$\n",
    "apply Multiplicative identity property\n",
    "$$ \\Rightarrow w_P = P^{-1} (X^T X)^{-1} X^T y $$\n",
    "substitute $(X^T X)^{-1} X^T y $ with $w$\n",
    "$$ \\Rightarrow w_P = P^{-1} w $$\n",
    "Q.E.D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What would be predicted values with $w_P$? \n",
    "$$ {y_{p}} = XP w_{p}$$\n",
    "substitute $w_p$ with the link we recieved $w_P = P^{-1} w $\n",
    "$$ \\Rightarrow {y_{p}} = XP P^{-1} w $$\n",
    "apply $A^{-1}A = AA^{-1} = I$\n",
    "$$ \\Rightarrow {y_{p}} = X I_{n} w $$\n",
    "apply Multiplicative identity property\n",
    "$$ \\Rightarrow {y_{p}} = X w $$\n",
    "applying our base equation $ y = X w $\n",
    "$$ \\Rightarrow {y_{p}} = y $$\n",
    "\n",
    "Q.E.D\n",
    "\n",
    "The predictions $y$ and $y_p$ are the same "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does that mean for the quality of linear regression if you measure it with RMSE?\n",
    "\n",
    "Since we prooved that the predfictions are identical before and after obfuscation the RMSE would be the same, since RMSE only looks at the final prediction versus the target that stayed as is (and n doesn't change as well).\n",
    "\n",
    "$$  RMSE = \\sqrt{\\frac{1}{n}\\Sigma_{i=1}^{n}{\\Big({target_i -prediction_i}\\Big)^2}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Linear Regression With Data Obfuscation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's prove Linear Regression can work computationally with the chosen obfuscation transformation.\n",
    "\n",
    "Build a procedure or a class that runs Linear Regression optionally with the obfuscation. You can use either a ready implementation of Linear Regression from sciki-learn or your own.\n",
    "\n",
    "Run Linear Regression for the original data and the obfuscated one, compare the predicted values and the RMSE, $R^2$ metric values. Is there any difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Procedure**\n",
    "\n",
    "- Create a square matrix $P$ of random numbers.\n",
    "- Check that it is invertible. If not, repeat the first point until we get an invertible matrix.\n",
    "- Use a class that runs Linear Regression and run it on X\n",
    "- Use $XP$ as the new feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a random matrix P\n",
    "rng = np.random.default_rng(seed=12345)\n",
    "P = rng.random(size=(X.shape[1], X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.97240014  1.76004024 -0.08309671  1.22285233]\n",
      " [ 0.14111106  0.32873452  1.02824721 -1.27752175]\n",
      " [ 0.8908452   0.90302415 -0.59501472 -0.23290483]\n",
      " [ 1.02530945 -1.81039816  0.24787878  0.46192295]]\n"
     ]
    }
   ],
   "source": [
    "# Checking the matrix $P$ is invertible\n",
    "p_invers = np.linalg.inv(P)\n",
    "print(p_invers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "luckily we found an inversible matrix. Yay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-9.43539012e-01  3.57495491e-02  1.64272726e-02 -2.60743659e-07\n",
      " -1.16902127e-02]\n",
      "RMSE: 0.34\n",
      "R2: 0.66\n"
     ]
    }
   ],
   "source": [
    "# Use a class that runs Linear Regression and run it on X\n",
    "X = df[['age', 'gender', 'income', 'family_members']].to_numpy()\n",
    "y = df['insurance_benefits'].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345)\n",
    "\n",
    "lr = MyLinearRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "print(lr.weights)\n",
    "\n",
    "y_test_pred = lr.predict(X_test)\n",
    "eval_regressor(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.94353857 -0.05589528  0.02537907  0.04940432  0.00151456]\n",
      "RMSE: 0.34\n",
      "R2: 0.66\n"
     ]
    }
   ],
   "source": [
    "# Use $XP$ as the new feature matrix\n",
    "XP_train = X_train.dot(P)\n",
    "XP_test = X_test.dot(P)\n",
    "\n",
    "lr.fit(XP_train, y_train)\n",
    "print(lr.weights)\n",
    "\n",
    "y_test_pred = lr.predict(XP_test)\n",
    "eval_regressor(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the scores haven't changed at all. It make sense because we prooved that the predictions are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First we uden a KNN model and test it for four combination of two cases:\n",
    "with / without scaling\n",
    "and\n",
    " Euclidean/Manhattan distance metric\n",
    "We found out that the scaling improves the model a great deal.since it’s balancing the weights of each feature and preventing from large scaling features like ‘salary’ to override the rest.\n",
    "Also the results of different distance metrics are not that different.\n",
    " \n",
    "Then we moved from regression to classification and set a new target combining two classes- clients who got insurance benefits (at least one) and those who didn’t receive any.\n",
    "We found out that the target classes are very imbalanced and used upsampling to fix it. We used a KNN classifier and compared it to a dummy model. The dummy model generated much lower scores where the worst model was with probability 0 and F1=0.\n",
    "\n",
    "We found out that the euclidean distance measure performs better in the KNN model than Manhattan and the optimal number of neighbors is 5-8. I guess that fewer neighbors are prowned to anomalies and hence less precise and big amount of neighbors have some extra instances that are not similar enough to the predicted instance and affect badly on the model. \n",
    " \n",
    "After the scaling we saw a different picture. The F1 score is much higher after the scaling. It went up to 0.98 and 0.99. It seems that after the scaling the better distance matric is manhattan\n",
    "\n",
    "Then we performed a linear regression and checked the influence of scaling on this model. The RMSE and F1 evaluations metrics haven't changed at all after scaling using the MaxAbsScaler. It seems that linear regression model isn’t sensitive to scaling like KNN for example.\n",
    "\n",
    "Then we obfuscated the data. We applied a random P transformation on our features matrix and discovered that applying the inverse of P will provide us with the original matrix X.\n",
    "We proved that data obfuscation can work with linear regression. We used analytical tools to prove that when applying $[P]^{-1}$ transformation on w we get $w_P$\n",
    "\n",
    "And from that we deduced that RMSE woukdn;t change since the predictions are identical before and after obfuscation. RMSE only looks at the final prediction versus the target that stayed as is .\n",
    "\n",
    "We tested Linear Regression with Data Obfuscation and discovered that the scores haven't changed at all. It makes sense because we proved that the predictions are the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type 'x' to check. Then press Shift+Enter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendices \n",
    "\n",
    "## Appendix A: Writing Formulas in Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write formulas in your Jupyter Notebook in a markup language provided by a high-quality publishing system called $\\LaTeX$ (pronounced \"Lah-tech\"), and they will look like formulas in textbooks.\n",
    "\n",
    "To put a formula in a text, put the dollar sign (\\\\$) before and after the formula's text e.g. $\\frac{1}{2} \\times \\frac{3}{2} = \\frac{3}{4}$ or $y = x^2, x \\ge 1$.\n",
    "\n",
    "If a formula should be in its own paragraph, put the double dollar sign (\\\\$\\\\$) before and after the formula text e.g.\n",
    "\n",
    "$$\n",
    "\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i.\n",
    "$$\n",
    "\n",
    "The markup language of [LaTeX](https://en.wikipedia.org/wiki/LaTeX) is very popular among people who use formulas in their articles, books and texts. It can be complex but its basics are easy. Check this two page [cheatsheet](http://tug.ctan.org/info/undergradmath/undergradmath.pdf) for learning how to compose the most common formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix B: Properties of Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices have many properties in Linear Algebra. A few of them are listed here which can help with the analytical proof in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td>Distributivity</td><td>$A(B+C)=AB+AC$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Non-commutativity</td><td>$AB \\neq BA$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Associative property of multiplication</td><td>$(AB)C = A(BC)$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Multiplicative identity property</td><td>$IA = AI = A$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td></td><td>$A^{-1}A = AA^{-1} = I$\n",
    "</td>\n",
    "</tr>    \n",
    "<tr>\n",
    "<td></td><td>$(AB)^{-1} = B^{-1}A^{-1}$</td>\n",
    "</tr>    \n",
    "<tr>\n",
    "<td>Reversivity of the transpose of a product of matrices,</td><td>$(AB)^T = B^TA^T$</td>\n",
    "</tr>    \n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 9677,
    "start_time": "2021-11-04T10:03:29.133Z"
   },
   {
    "duration": 13144,
    "start_time": "2021-11-04T10:03:38.813Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-04T10:03:51.961Z"
   },
   {
    "duration": 29,
    "start_time": "2021-11-04T10:03:51.970Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-04T10:03:52.002Z"
   },
   {
    "duration": 29,
    "start_time": "2021-11-04T10:03:52.012Z"
   },
   {
    "duration": 11,
    "start_time": "2021-11-04T10:03:52.044Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-04T10:03:52.062Z"
   },
   {
    "duration": 54,
    "start_time": "2021-11-04T10:03:52.071Z"
   },
   {
    "duration": 35,
    "start_time": "2021-11-04T10:03:52.129Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-04T10:03:52.167Z"
   },
   {
    "duration": 53,
    "start_time": "2021-11-04T10:03:52.177Z"
   },
   {
    "duration": 624,
    "start_time": "2021-11-04T10:03:52.232Z"
   },
   {
    "duration": 8775,
    "start_time": "2021-11-04T10:03:52.859Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-04T10:04:01.636Z"
   },
   {
    "duration": 18,
    "start_time": "2021-11-04T10:04:01.642Z"
   },
   {
    "duration": 26,
    "start_time": "2021-11-04T10:04:01.662Z"
   },
   {
    "duration": 11,
    "start_time": "2021-11-04T10:04:01.691Z"
   },
   {
    "duration": 29,
    "start_time": "2021-11-04T10:04:01.705Z"
   },
   {
    "duration": 16,
    "start_time": "2021-11-04T10:04:01.737Z"
   },
   {
    "duration": 23,
    "start_time": "2021-11-04T10:04:01.755Z"
   },
   {
    "duration": 57,
    "start_time": "2021-11-04T10:04:01.780Z"
   },
   {
    "duration": 89,
    "start_time": "2021-11-04T10:04:01.843Z"
   },
   {
    "duration": 20,
    "start_time": "2021-11-04T10:04:01.935Z"
   },
   {
    "duration": 23,
    "start_time": "2021-11-04T10:04:01.957Z"
   },
   {
    "duration": 47,
    "start_time": "2021-11-04T10:04:01.982Z"
   },
   {
    "duration": 45,
    "start_time": "2021-11-04T10:04:02.031Z"
   },
   {
    "duration": 36,
    "start_time": "2021-11-04T10:04:02.078Z"
   },
   {
    "duration": 10,
    "start_time": "2021-11-04T10:04:02.116Z"
   },
   {
    "duration": 23,
    "start_time": "2021-11-04T10:04:02.128Z"
   },
   {
    "duration": 9,
    "start_time": "2021-11-04T10:04:02.153Z"
   },
   {
    "duration": 13,
    "start_time": "2021-11-04T10:04:02.165Z"
   },
   {
    "duration": 39,
    "start_time": "2021-11-04T10:04:02.181Z"
   },
   {
    "duration": 8,
    "start_time": "2021-11-04T10:04:02.223Z"
   },
   {
    "duration": 9,
    "start_time": "2021-11-04T10:04:02.233Z"
   },
   {
    "duration": 5281,
    "start_time": "2021-11-04T10:04:02.244Z"
   },
   {
    "duration": 8,
    "start_time": "2021-11-04T10:04:07.528Z"
   },
   {
    "duration": 6334,
    "start_time": "2021-11-04T10:04:07.538Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-04T10:04:13.874Z"
   },
   {
    "duration": 8,
    "start_time": "2021-11-04T10:04:13.884Z"
   },
   {
    "duration": 40,
    "start_time": "2021-11-04T10:04:13.894Z"
   },
   {
    "duration": 85,
    "start_time": "2021-11-04T10:04:13.938Z"
   },
   {
    "duration": 96,
    "start_time": "2021-11-04T10:04:14.027Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-04T10:04:14.127Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-04T10:04:14.222Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-04T10:04:14.229Z"
   },
   {
    "duration": 9,
    "start_time": "2021-11-04T10:04:14.238Z"
   },
   {
    "duration": 77,
    "start_time": "2021-11-04T10:04:14.251Z"
   },
   {
    "duration": 9,
    "start_time": "2021-11-04T10:04:14.332Z"
   },
   {
    "duration": 9,
    "start_time": "2021-11-04T10:04:14.423Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-04T10:04:14.435Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-04T10:04:14.444Z"
   },
   {
    "duration": 15,
    "start_time": "2021-11-04T10:04:14.451Z"
   },
   {
    "duration": 19,
    "start_time": "2021-11-04T10:04:14.523Z"
   },
   {
    "duration": 13,
    "start_time": "2021-11-04T10:04:14.625Z"
   },
   {
    "duration": 3,
    "start_time": "2021-11-04T11:20:37.288Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-04T11:20:45.508Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-04T11:20:52.436Z"
   },
   {
    "duration": 804,
    "start_time": "2021-11-04T11:22:27.022Z"
   },
   {
    "duration": 3964,
    "start_time": "2021-11-04T11:22:35.752Z"
   },
   {
    "duration": 1696,
    "start_time": "2021-11-04T11:22:39.720Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-04T11:22:41.423Z"
   },
   {
    "duration": 27,
    "start_time": "2021-11-04T11:22:41.431Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-04T11:22:41.460Z"
   },
   {
    "duration": 22,
    "start_time": "2021-11-04T11:22:41.468Z"
   },
   {
    "duration": 38,
    "start_time": "2021-11-04T11:22:41.492Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-04T11:22:41.535Z"
   },
   {
    "duration": 23,
    "start_time": "2021-11-04T11:22:41.544Z"
   },
   {
    "duration": 58,
    "start_time": "2021-11-04T11:22:41.570Z"
   },
   {
    "duration": 8,
    "start_time": "2021-11-04T11:22:41.631Z"
   },
   {
    "duration": 21,
    "start_time": "2021-11-04T11:22:41.641Z"
   },
   {
    "duration": 670,
    "start_time": "2021-11-04T11:22:41.665Z"
   },
   {
    "duration": 8710,
    "start_time": "2021-11-04T11:22:42.338Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-04T11:22:51.050Z"
   },
   {
    "duration": 10,
    "start_time": "2021-11-04T11:22:51.056Z"
   },
   {
    "duration": 23,
    "start_time": "2021-11-04T11:22:51.068Z"
   },
   {
    "duration": 10,
    "start_time": "2021-11-04T11:22:51.093Z"
   },
   {
    "duration": 21,
    "start_time": "2021-11-04T11:22:51.125Z"
   },
   {
    "duration": 20,
    "start_time": "2021-11-04T11:22:51.149Z"
   },
   {
    "duration": 20,
    "start_time": "2021-11-04T11:22:51.171Z"
   },
   {
    "duration": 26,
    "start_time": "2021-11-04T11:22:51.223Z"
   },
   {
    "duration": 19,
    "start_time": "2021-11-04T11:22:51.255Z"
   },
   {
    "duration": 44,
    "start_time": "2021-11-04T11:22:51.278Z"
   },
   {
    "duration": 27,
    "start_time": "2021-11-04T11:22:51.324Z"
   },
   {
    "duration": 21,
    "start_time": "2021-11-04T11:22:51.354Z"
   },
   {
    "duration": 58,
    "start_time": "2021-11-04T11:22:51.378Z"
   },
   {
    "duration": 22,
    "start_time": "2021-11-04T11:22:51.439Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-04T11:22:51.464Z"
   },
   {
    "duration": 23,
    "start_time": "2021-11-04T11:22:51.473Z"
   },
   {
    "duration": 24,
    "start_time": "2021-11-04T11:22:51.498Z"
   },
   {
    "duration": 12,
    "start_time": "2021-11-04T11:22:51.526Z"
   },
   {
    "duration": 8,
    "start_time": "2021-11-04T11:22:51.541Z"
   },
   {
    "duration": 17,
    "start_time": "2021-11-04T11:22:51.552Z"
   },
   {
    "duration": 37,
    "start_time": "2021-11-04T11:22:51.571Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-04T11:22:51.631Z"
   },
   {
    "duration": 16,
    "start_time": "2021-11-04T11:22:51.640Z"
   },
   {
    "duration": 830,
    "start_time": "2021-11-04T11:22:51.659Z"
   },
   {
    "duration": -312,
    "start_time": "2021-11-04T11:22:52.804Z"
   },
   {
    "duration": -384,
    "start_time": "2021-11-04T11:22:52.878Z"
   },
   {
    "duration": -392,
    "start_time": "2021-11-04T11:22:52.887Z"
   },
   {
    "duration": -398,
    "start_time": "2021-11-04T11:22:52.895Z"
   },
   {
    "duration": -406,
    "start_time": "2021-11-04T11:22:52.904Z"
   },
   {
    "duration": -415,
    "start_time": "2021-11-04T11:22:52.914Z"
   },
   {
    "duration": -416,
    "start_time": "2021-11-04T11:22:52.917Z"
   },
   {
    "duration": -422,
    "start_time": "2021-11-04T11:22:52.924Z"
   },
   {
    "duration": -426,
    "start_time": "2021-11-04T11:22:52.929Z"
   },
   {
    "duration": -427,
    "start_time": "2021-11-04T11:22:52.932Z"
   },
   {
    "duration": -432,
    "start_time": "2021-11-04T11:22:52.938Z"
   },
   {
    "duration": -438,
    "start_time": "2021-11-04T11:22:52.945Z"
   },
   {
    "duration": -443,
    "start_time": "2021-11-04T11:22:52.951Z"
   },
   {
    "duration": -446,
    "start_time": "2021-11-04T11:22:52.957Z"
   },
   {
    "duration": -450,
    "start_time": "2021-11-04T11:22:52.963Z"
   },
   {
    "duration": -452,
    "start_time": "2021-11-04T11:22:52.966Z"
   },
   {
    "duration": -493,
    "start_time": "2021-11-04T11:22:53.009Z"
   },
   {
    "duration": -502,
    "start_time": "2021-11-04T11:22:53.019Z"
   },
   {
    "duration": -511,
    "start_time": "2021-11-04T11:22:53.030Z"
   },
   {
    "duration": -543,
    "start_time": "2021-11-04T11:22:53.063Z"
   },
   {
    "duration": 3798,
    "start_time": "2021-11-04T11:24:25.179Z"
   },
   {
    "duration": 1602,
    "start_time": "2021-11-04T11:24:28.980Z"
   },
   {
    "duration": 3,
    "start_time": "2021-11-04T11:24:30.585Z"
   },
   {
    "duration": 34,
    "start_time": "2021-11-04T11:24:30.591Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-04T11:24:30.627Z"
   },
   {
    "duration": 23,
    "start_time": "2021-11-04T11:24:30.635Z"
   },
   {
    "duration": 17,
    "start_time": "2021-11-04T11:24:30.661Z"
   },
   {
    "duration": 42,
    "start_time": "2021-11-04T11:24:30.682Z"
   },
   {
    "duration": 13,
    "start_time": "2021-11-04T11:24:30.727Z"
   },
   {
    "duration": 45,
    "start_time": "2021-11-04T11:24:30.743Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-04T11:24:30.791Z"
   },
   {
    "duration": 19,
    "start_time": "2021-11-04T11:24:30.823Z"
   },
   {
    "duration": 682,
    "start_time": "2021-11-04T11:24:30.844Z"
   },
   {
    "duration": 8254,
    "start_time": "2021-11-04T11:24:31.529Z"
   },
   {
    "duration": 3,
    "start_time": "2021-11-04T11:24:39.786Z"
   },
   {
    "duration": 30,
    "start_time": "2021-11-04T11:24:39.792Z"
   },
   {
    "duration": 21,
    "start_time": "2021-11-04T11:24:39.825Z"
   },
   {
    "duration": 8,
    "start_time": "2021-11-04T11:24:39.848Z"
   },
   {
    "duration": 18,
    "start_time": "2021-11-04T11:24:39.859Z"
   },
   {
    "duration": 52,
    "start_time": "2021-11-04T11:24:39.880Z"
   },
   {
    "duration": 20,
    "start_time": "2021-11-04T11:24:39.935Z"
   },
   {
    "duration": 21,
    "start_time": "2021-11-04T11:24:39.958Z"
   },
   {
    "duration": 54,
    "start_time": "2021-11-04T11:24:39.987Z"
   },
   {
    "duration": 21,
    "start_time": "2021-11-04T11:24:40.045Z"
   },
   {
    "duration": 56,
    "start_time": "2021-11-04T11:24:40.070Z"
   },
   {
    "duration": 18,
    "start_time": "2021-11-04T11:24:40.129Z"
   },
   {
    "duration": 19,
    "start_time": "2021-11-04T11:24:40.149Z"
   },
   {
    "duration": 17,
    "start_time": "2021-11-04T11:24:40.171Z"
   },
   {
    "duration": 34,
    "start_time": "2021-11-04T11:24:40.191Z"
   },
   {
    "duration": 11,
    "start_time": "2021-11-04T11:24:40.228Z"
   },
   {
    "duration": 12,
    "start_time": "2021-11-04T11:24:40.241Z"
   },
   {
    "duration": 9,
    "start_time": "2021-11-04T11:24:40.255Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-04T11:24:40.266Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-04T11:24:40.274Z"
   },
   {
    "duration": 70,
    "start_time": "2021-11-04T11:24:40.281Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-04T11:24:40.353Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-04T11:24:40.363Z"
   },
   {
    "duration": 53,
    "start_time": "2021-11-04T11:24:40.373Z"
   },
   {
    "duration": 443,
    "start_time": "2021-11-04T11:24:40.428Z"
   },
   {
    "duration": -144,
    "start_time": "2021-11-04T11:24:41.018Z"
   },
   {
    "duration": -150,
    "start_time": "2021-11-04T11:24:41.025Z"
   },
   {
    "duration": -154,
    "start_time": "2021-11-04T11:24:41.031Z"
   },
   {
    "duration": -159,
    "start_time": "2021-11-04T11:24:41.037Z"
   },
   {
    "duration": -163,
    "start_time": "2021-11-04T11:24:41.042Z"
   },
   {
    "duration": -168,
    "start_time": "2021-11-04T11:24:41.049Z"
   },
   {
    "duration": -171,
    "start_time": "2021-11-04T11:24:41.053Z"
   },
   {
    "duration": -173,
    "start_time": "2021-11-04T11:24:41.056Z"
   },
   {
    "duration": -176,
    "start_time": "2021-11-04T11:24:41.060Z"
   },
   {
    "duration": -188,
    "start_time": "2021-11-04T11:24:41.074Z"
   },
   {
    "duration": -190,
    "start_time": "2021-11-04T11:24:41.077Z"
   },
   {
    "duration": -193,
    "start_time": "2021-11-04T11:24:41.081Z"
   },
   {
    "duration": -194,
    "start_time": "2021-11-04T11:24:41.084Z"
   },
   {
    "duration": -199,
    "start_time": "2021-11-04T11:24:41.090Z"
   },
   {
    "duration": -202,
    "start_time": "2021-11-04T11:24:41.094Z"
   },
   {
    "duration": -204,
    "start_time": "2021-11-04T11:24:41.097Z"
   },
   {
    "duration": -213,
    "start_time": "2021-11-04T11:24:41.108Z"
   },
   {
    "duration": -216,
    "start_time": "2021-11-04T11:24:41.112Z"
   },
   {
    "duration": -223,
    "start_time": "2021-11-04T11:24:41.120Z"
   },
   {
    "duration": 3948,
    "start_time": "2021-11-04T11:30:27.649Z"
   },
   {
    "duration": 1716,
    "start_time": "2021-11-04T11:30:31.599Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-04T11:30:33.318Z"
   },
   {
    "duration": 23,
    "start_time": "2021-11-04T11:30:33.327Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-04T11:30:33.352Z"
   },
   {
    "duration": 22,
    "start_time": "2021-11-04T11:30:33.362Z"
   },
   {
    "duration": 48,
    "start_time": "2021-11-04T11:30:33.387Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-04T11:30:33.441Z"
   },
   {
    "duration": 14,
    "start_time": "2021-11-04T11:30:33.449Z"
   },
   {
    "duration": 69,
    "start_time": "2021-11-04T11:30:33.466Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-04T11:30:33.538Z"
   },
   {
    "duration": 17,
    "start_time": "2021-11-04T11:30:33.547Z"
   },
   {
    "duration": 646,
    "start_time": "2021-11-04T11:30:33.567Z"
   },
   {
    "duration": 8493,
    "start_time": "2021-11-04T11:30:34.215Z"
   },
   {
    "duration": 12,
    "start_time": "2021-11-04T11:30:42.711Z"
   },
   {
    "duration": 10,
    "start_time": "2021-11-04T11:30:42.725Z"
   },
   {
    "duration": 19,
    "start_time": "2021-11-04T11:30:42.738Z"
   },
   {
    "duration": 9,
    "start_time": "2021-11-04T11:30:42.759Z"
   },
   {
    "duration": 52,
    "start_time": "2021-11-04T11:30:42.770Z"
   },
   {
    "duration": 15,
    "start_time": "2021-11-04T11:30:42.825Z"
   },
   {
    "duration": 30,
    "start_time": "2021-11-04T11:30:42.842Z"
   },
   {
    "duration": 22,
    "start_time": "2021-11-04T11:30:42.875Z"
   },
   {
    "duration": 19,
    "start_time": "2021-11-04T11:30:42.924Z"
   },
   {
    "duration": 20,
    "start_time": "2021-11-04T11:30:42.947Z"
   },
   {
    "duration": 28,
    "start_time": "2021-11-04T11:30:42.969Z"
   },
   {
    "duration": 20,
    "start_time": "2021-11-04T11:30:43.023Z"
   },
   {
    "duration": 36,
    "start_time": "2021-11-04T11:30:43.046Z"
   },
   {
    "duration": 20,
    "start_time": "2021-11-04T11:30:43.085Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-04T11:30:43.123Z"
   },
   {
    "duration": 15,
    "start_time": "2021-11-04T11:30:43.131Z"
   },
   {
    "duration": 13,
    "start_time": "2021-11-04T11:30:43.149Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-04T11:30:43.165Z"
   },
   {
    "duration": 16,
    "start_time": "2021-11-04T11:30:43.174Z"
   },
   {
    "duration": 9,
    "start_time": "2021-11-04T11:30:43.192Z"
   },
   {
    "duration": 47,
    "start_time": "2021-11-04T11:30:43.203Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-04T11:30:43.252Z"
   },
   {
    "duration": 15,
    "start_time": "2021-11-04T11:30:43.261Z"
   },
   {
    "duration": 10,
    "start_time": "2021-11-04T11:30:43.278Z"
   },
   {
    "duration": 5496,
    "start_time": "2021-11-04T11:30:43.290Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-04T11:30:48.789Z"
   },
   {
    "duration": 5905,
    "start_time": "2021-11-04T11:30:48.799Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-04T11:30:54.706Z"
   },
   {
    "duration": 14,
    "start_time": "2021-11-04T11:30:54.715Z"
   },
   {
    "duration": 21,
    "start_time": "2021-11-04T11:30:54.731Z"
   },
   {
    "duration": 74,
    "start_time": "2021-11-04T11:30:54.756Z"
   },
   {
    "duration": 98,
    "start_time": "2021-11-04T11:30:54.834Z"
   },
   {
    "duration": 89,
    "start_time": "2021-11-04T11:30:54.936Z"
   },
   {
    "duration": 95,
    "start_time": "2021-11-04T11:30:55.029Z"
   },
   {
    "duration": 8,
    "start_time": "2021-11-04T11:30:55.127Z"
   },
   {
    "duration": 9,
    "start_time": "2021-11-04T11:30:55.137Z"
   },
   {
    "duration": 74,
    "start_time": "2021-11-04T11:30:55.149Z"
   },
   {
    "duration": 8,
    "start_time": "2021-11-04T11:30:55.226Z"
   },
   {
    "duration": 90,
    "start_time": "2021-11-04T11:30:55.237Z"
   },
   {
    "duration": 13,
    "start_time": "2021-11-04T11:30:55.329Z"
   },
   {
    "duration": 8,
    "start_time": "2021-11-04T11:30:55.345Z"
   },
   {
    "duration": 10,
    "start_time": "2021-11-04T11:30:55.355Z"
   },
   {
    "duration": 73,
    "start_time": "2021-11-04T11:30:55.367Z"
   },
   {
    "duration": 13,
    "start_time": "2021-11-04T11:30:55.524Z"
   },
   {
    "duration": 3776,
    "start_time": "2021-11-04T11:31:46.701Z"
   },
   {
    "duration": 1680,
    "start_time": "2021-11-04T11:31:50.480Z"
   },
   {
    "duration": 3,
    "start_time": "2021-11-04T11:31:52.163Z"
   },
   {
    "duration": 24,
    "start_time": "2021-11-04T11:31:52.169Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-04T11:31:52.196Z"
   },
   {
    "duration": 33,
    "start_time": "2021-11-04T11:31:52.203Z"
   },
   {
    "duration": 15,
    "start_time": "2021-11-04T11:31:52.238Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-04T11:31:52.257Z"
   },
   {
    "duration": 15,
    "start_time": "2021-11-04T11:31:52.265Z"
   },
   {
    "duration": 65,
    "start_time": "2021-11-04T11:31:52.282Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-04T11:31:52.350Z"
   },
   {
    "duration": 20,
    "start_time": "2021-11-04T11:31:52.359Z"
   },
   {
    "duration": 680,
    "start_time": "2021-11-04T11:31:52.381Z"
   },
   {
    "duration": 8837,
    "start_time": "2021-11-04T11:31:53.063Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-04T11:32:01.903Z"
   },
   {
    "duration": 15,
    "start_time": "2021-11-04T11:32:01.909Z"
   },
   {
    "duration": 25,
    "start_time": "2021-11-04T11:32:01.926Z"
   },
   {
    "duration": 13,
    "start_time": "2021-11-04T11:32:01.954Z"
   },
   {
    "duration": 19,
    "start_time": "2021-11-04T11:32:01.970Z"
   },
   {
    "duration": 39,
    "start_time": "2021-11-04T11:32:01.992Z"
   },
   {
    "duration": 18,
    "start_time": "2021-11-04T11:32:02.034Z"
   },
   {
    "duration": 19,
    "start_time": "2021-11-04T11:32:02.055Z"
   },
   {
    "duration": 59,
    "start_time": "2021-11-04T11:32:02.080Z"
   },
   {
    "duration": 19,
    "start_time": "2021-11-04T11:32:02.143Z"
   },
   {
    "duration": 26,
    "start_time": "2021-11-04T11:32:02.164Z"
   },
   {
    "duration": 40,
    "start_time": "2021-11-04T11:32:02.192Z"
   },
   {
    "duration": 31,
    "start_time": "2021-11-04T11:32:02.235Z"
   },
   {
    "duration": 20,
    "start_time": "2021-11-04T11:32:02.268Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-04T11:32:02.291Z"
   },
   {
    "duration": 8,
    "start_time": "2021-11-04T11:32:02.323Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-04T11:32:02.333Z"
   },
   {
    "duration": 22,
    "start_time": "2021-11-04T11:32:02.342Z"
   },
   {
    "duration": 10,
    "start_time": "2021-11-04T11:32:02.366Z"
   },
   {
    "duration": 8,
    "start_time": "2021-11-04T11:32:02.379Z"
   },
   {
    "duration": 54,
    "start_time": "2021-11-04T11:32:02.391Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-04T11:32:02.448Z"
   },
   {
    "duration": 27,
    "start_time": "2021-11-04T11:32:02.456Z"
   },
   {
    "duration": 20,
    "start_time": "2021-11-04T11:32:02.486Z"
   },
   {
    "duration": 8106,
    "start_time": "2021-11-04T11:32:02.509Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-04T11:32:10.623Z"
   },
   {
    "duration": 9223,
    "start_time": "2021-11-04T11:32:10.633Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-04T11:32:19.860Z"
   },
   {
    "duration": 12,
    "start_time": "2021-11-04T11:32:19.868Z"
   },
   {
    "duration": 42,
    "start_time": "2021-11-04T11:32:19.882Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-04T11:32:19.928Z"
   },
   {
    "duration": 12,
    "start_time": "2021-11-04T11:32:20.024Z"
   },
   {
    "duration": 8,
    "start_time": "2021-11-04T11:32:20.124Z"
   },
   {
    "duration": 90,
    "start_time": "2021-11-04T11:32:20.136Z"
   },
   {
    "duration": 15,
    "start_time": "2021-11-04T11:32:20.228Z"
   },
   {
    "duration": 10,
    "start_time": "2021-11-04T11:32:20.246Z"
   },
   {
    "duration": 64,
    "start_time": "2021-11-04T11:32:20.259Z"
   },
   {
    "duration": 95,
    "start_time": "2021-11-04T11:32:20.328Z"
   },
   {
    "duration": 35,
    "start_time": "2021-11-04T11:32:20.426Z"
   },
   {
    "duration": 22,
    "start_time": "2021-11-04T11:32:20.463Z"
   },
   {
    "duration": 17,
    "start_time": "2021-11-04T11:32:20.487Z"
   },
   {
    "duration": 17,
    "start_time": "2021-11-04T11:32:20.507Z"
   },
   {
    "duration": 97,
    "start_time": "2021-11-04T11:32:20.526Z"
   },
   {
    "duration": 96,
    "start_time": "2021-11-04T11:32:20.627Z"
   },
   {
    "duration": 8879,
    "start_time": "2021-11-04T11:38:28.169Z"
   },
   {
    "duration": 8397,
    "start_time": "2021-11-04T11:38:43.808Z"
   },
   {
    "duration": 8297,
    "start_time": "2021-11-04T11:43:19.477Z"
   },
   {
    "duration": 8052,
    "start_time": "2021-11-04T11:43:57.379Z"
   },
   {
    "duration": 3061,
    "start_time": "2021-11-04T11:51:31.575Z"
   },
   {
    "duration": 3111,
    "start_time": "2021-11-04T11:51:50.199Z"
   },
   {
    "duration": 238,
    "start_time": "2021-11-04T12:39:55.113Z"
   },
   {
    "duration": 4969,
    "start_time": "2021-11-04T12:41:46.305Z"
   },
   {
    "duration": 1848,
    "start_time": "2021-11-04T12:41:51.277Z"
   },
   {
    "duration": 3,
    "start_time": "2021-11-04T12:41:53.128Z"
   },
   {
    "duration": 25,
    "start_time": "2021-11-04T12:41:53.134Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-04T12:41:53.161Z"
   },
   {
    "duration": 32,
    "start_time": "2021-11-04T12:41:53.168Z"
   },
   {
    "duration": 38,
    "start_time": "2021-11-04T12:41:53.202Z"
   },
   {
    "duration": 10,
    "start_time": "2021-11-04T12:41:53.244Z"
   },
   {
    "duration": 17,
    "start_time": "2021-11-04T12:41:53.257Z"
   },
   {
    "duration": 64,
    "start_time": "2021-11-04T12:41:53.276Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-04T12:41:53.342Z"
   },
   {
    "duration": 19,
    "start_time": "2021-11-04T12:41:53.351Z"
   },
   {
    "duration": 1972,
    "start_time": "2021-11-04T12:41:53.372Z"
   },
   {
    "duration": 8885,
    "start_time": "2021-11-04T12:41:55.347Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-04T12:42:04.235Z"
   },
   {
    "duration": 8,
    "start_time": "2021-11-04T12:42:04.242Z"
   },
   {
    "duration": 20,
    "start_time": "2021-11-04T12:42:04.252Z"
   },
   {
    "duration": 8,
    "start_time": "2021-11-04T12:42:04.275Z"
   },
   {
    "duration": 53,
    "start_time": "2021-11-04T12:42:04.285Z"
   },
   {
    "duration": 14,
    "start_time": "2021-11-04T12:42:04.340Z"
   },
   {
    "duration": 20,
    "start_time": "2021-11-04T12:42:04.356Z"
   },
   {
    "duration": 57,
    "start_time": "2021-11-04T12:42:04.378Z"
   },
   {
    "duration": 20,
    "start_time": "2021-11-04T12:42:04.442Z"
   },
   {
    "duration": 20,
    "start_time": "2021-11-04T12:42:04.466Z"
   },
   {
    "duration": 52,
    "start_time": "2021-11-04T12:42:04.488Z"
   },
   {
    "duration": 20,
    "start_time": "2021-11-04T12:42:04.543Z"
   },
   {
    "duration": 19,
    "start_time": "2021-11-04T12:42:04.566Z"
   },
   {
    "duration": 53,
    "start_time": "2021-11-04T12:42:04.588Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-04T12:42:04.644Z"
   },
   {
    "duration": 11,
    "start_time": "2021-11-04T12:42:04.654Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-04T12:42:04.667Z"
   },
   {
    "duration": 45,
    "start_time": "2021-11-04T12:42:04.677Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-04T12:42:04.725Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-04T12:42:04.732Z"
   },
   {
    "duration": 35,
    "start_time": "2021-11-04T12:42:04.739Z"
   },
   {
    "duration": 48,
    "start_time": "2021-11-04T12:42:04.776Z"
   },
   {
    "duration": 11,
    "start_time": "2021-11-04T12:42:04.828Z"
   },
   {
    "duration": 32,
    "start_time": "2021-11-04T12:42:04.842Z"
   },
   {
    "duration": 9105,
    "start_time": "2021-11-04T12:42:04.877Z"
   },
   {
    "duration": 8,
    "start_time": "2021-11-04T12:42:13.985Z"
   },
   {
    "duration": 4308,
    "start_time": "2021-11-04T12:42:13.995Z"
   },
   {
    "duration": 17,
    "start_time": "2021-11-04T12:42:18.305Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-04T12:42:18.325Z"
   },
   {
    "duration": 24,
    "start_time": "2021-11-04T12:42:18.331Z"
   },
   {
    "duration": 66,
    "start_time": "2021-11-04T12:42:18.359Z"
   },
   {
    "duration": 101,
    "start_time": "2021-11-04T12:42:18.429Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-04T12:42:18.533Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-04T12:42:18.623Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-04T12:42:18.630Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-04T12:42:18.641Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-04T12:42:18.723Z"
   },
   {
    "duration": 90,
    "start_time": "2021-11-04T12:42:18.733Z"
   },
   {
    "duration": 10,
    "start_time": "2021-11-04T12:42:18.826Z"
   },
   {
    "duration": 10,
    "start_time": "2021-11-04T12:42:18.838Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-04T12:42:18.851Z"
   },
   {
    "duration": 9,
    "start_time": "2021-11-04T12:42:18.858Z"
   },
   {
    "duration": 18,
    "start_time": "2021-11-04T12:42:18.924Z"
   },
   {
    "duration": 12,
    "start_time": "2021-11-04T12:42:19.023Z"
   },
   {
    "duration": 60,
    "start_time": "2021-11-04T20:41:26.413Z"
   },
   {
    "duration": 7782,
    "start_time": "2021-11-04T21:06:27.035Z"
   },
   {
    "duration": 7611,
    "start_time": "2021-11-04T21:06:34.820Z"
   },
   {
    "duration": 3,
    "start_time": "2021-11-04T21:06:42.433Z"
   },
   {
    "duration": 25,
    "start_time": "2021-11-04T21:06:42.438Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-04T21:06:42.464Z"
   },
   {
    "duration": 15,
    "start_time": "2021-11-04T21:06:42.470Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-04T21:06:42.486Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-04T21:06:42.495Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-04T21:06:42.500Z"
   },
   {
    "duration": 20,
    "start_time": "2021-11-04T21:06:42.509Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-04T21:06:42.530Z"
   },
   {
    "duration": 12,
    "start_time": "2021-11-04T21:06:42.536Z"
   },
   {
    "duration": 325,
    "start_time": "2021-11-04T21:06:42.549Z"
   },
   {
    "duration": 5137,
    "start_time": "2021-11-04T21:06:42.876Z"
   },
   {
    "duration": 2,
    "start_time": "2021-11-04T21:06:48.015Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-04T21:06:48.019Z"
   },
   {
    "duration": 15,
    "start_time": "2021-11-04T21:06:48.027Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-04T21:06:48.044Z"
   },
   {
    "duration": 12,
    "start_time": "2021-11-04T21:06:48.051Z"
   },
   {
    "duration": 34,
    "start_time": "2021-11-04T21:06:48.065Z"
   },
   {
    "duration": 16,
    "start_time": "2021-11-04T21:06:48.101Z"
   },
   {
    "duration": 13,
    "start_time": "2021-11-04T21:06:48.118Z"
   },
   {
    "duration": 13,
    "start_time": "2021-11-04T21:06:48.135Z"
   },
   {
    "duration": 13,
    "start_time": "2021-11-04T21:06:48.151Z"
   },
   {
    "duration": 12,
    "start_time": "2021-11-04T21:06:48.192Z"
   },
   {
    "duration": 18,
    "start_time": "2021-11-04T21:06:48.206Z"
   },
   {
    "duration": 11,
    "start_time": "2021-11-04T21:06:48.226Z"
   },
   {
    "duration": 13,
    "start_time": "2021-11-04T21:06:48.239Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-04T21:06:48.253Z"
   },
   {
    "duration": 34,
    "start_time": "2021-11-04T21:06:48.258Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-04T21:06:48.294Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-04T21:06:48.300Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-04T21:06:48.308Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-04T21:06:48.313Z"
   },
   {
    "duration": 25,
    "start_time": "2021-11-04T21:06:48.320Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-04T21:06:48.346Z"
   },
   {
    "duration": 37,
    "start_time": "2021-11-04T21:06:48.354Z"
   },
   {
    "duration": 18,
    "start_time": "2021-11-04T21:06:48.393Z"
   },
   {
    "duration": 4427,
    "start_time": "2021-11-04T21:06:48.413Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-04T21:06:52.842Z"
   },
   {
    "duration": 1620,
    "start_time": "2021-11-04T21:06:52.848Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-04T21:06:54.470Z"
   },
   {
    "duration": 19,
    "start_time": "2021-11-04T21:06:54.475Z"
   },
   {
    "duration": 12,
    "start_time": "2021-11-04T21:06:54.496Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-04T21:06:54.510Z"
   },
   {
    "duration": 80,
    "start_time": "2021-11-04T21:06:54.516Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-04T21:06:54.598Z"
   },
   {
    "duration": 88,
    "start_time": "2021-11-04T21:06:54.605Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-04T21:06:54.695Z"
   },
   {
    "duration": 91,
    "start_time": "2021-11-04T21:06:54.702Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-04T21:06:54.795Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-04T21:06:54.803Z"
   },
   {
    "duration": 83,
    "start_time": "2021-11-04T21:06:54.809Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-04T21:06:54.894Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-04T21:06:54.899Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-04T21:06:54.907Z"
   },
   {
    "duration": 77,
    "start_time": "2021-11-04T21:06:54.915Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-04T21:06:54.993Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
